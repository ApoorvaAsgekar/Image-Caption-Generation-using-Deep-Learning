# Image-Caption-Generation-using-Deep-Learning

This projects aim was to develop a model capable of automatically generating captions to describe images, which presents a significantly more complex task compared to image classification and object recognition. The project utilizes the Flickr 8k dataset, consisting of 8000 images, each paired with five natural language captions. The dataset structure has each image having multiple captions. My approach involved employing both Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN). The pre-trained CNN acts as an image encoder, with its last hidden layer serving as input to the RNN, acting as a decoder to generate sentences. However, challenges arised as the generated sentences may sometimes lose track or inaccurately predict the original image content, potentially due to common phrases in the dataset leading to weakly related sentences to the input image.

Computer vision aspect plays a crucial role in understanding and interpreting the visual content of images. This involves the application of Convolutional Neural Networks (CNNs), which are deep learning models specifically designed for image processing tasks. In the context of image caption generation, the computer vision aspect begins with the input images. CNNs are employed to extract meaningful features from these images by processing them through multiple convolutional and pooling layers. These layers enable the network to learn hierarchical representations of the image, capturing low-level features such as edges and textures, and gradually progressing to higher-level features such as shapes and objects.

Once the CNN processes the input image, it encodes it into a fixed-dimensional vector representation, often referred to as a feature vector or embedding. This embedding encapsulates the visual information of the image in a format that can be further utilized by the caption generation component.

The feature vector extracted by the CNN serves as input to the Recurrent Neural Network (RNN) component of the model. The RNN is responsible for generating natural language captions based on the visual information provided by the CNN. It operates sequentially, generating words one at a time while taking into account the context of previously generated words. This sequential generation process allows the model to produce coherent and contextually relevant captions that describe the content of the input image.

